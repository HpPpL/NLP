# bdbr = 'При обработке естественного языка, тексты разбиваются на предложения. В nltk принято окружать предложения токенами. Для выполнения этой процедуры существует набор функций препроцессинга. Они также возвращают генераторы, поэтому являются "одноразовыми":'
# hui = pd.DataFrame()
# hui['Text'] = sent_tokenize(bdbr)
# hui['Text'] = hui['Text'].apply(lambda txt: data_preprocessing(txt, 1))
# bebra = hui['Text'].to_list()
#
# for i in range(len(bebra)):
#     bebra[i] = ' '.join(bebra[i])
#
# bbra =['fsdf sdf sdfsd fsd fsdf sdf', 'sdf sfd sdf sdf fd']
# bra = ['I am going to test Covid', 'It seems ABC hospital doing the Covid test', 'Covaxin is still in WIP phase']
# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), stop_words = stop_words)
# vectorizer.fit(bebra)
# tfidf = vectorizer.transform(bebra)
# vectors = vectorizer.get_feature_names()
# smatrix = vectorizer.transform(bebra)
# dense = smatrix.todense()
# dense_list = dense.tolist()
# df_tfidf = pd.DataFrame(dense_list,columns=vectors)
# # df_tfidf
# lssst = []
# gavno = []
# res =
# for i in range(df_tfidf.shape[0]):
#     lssst.append(df_tfidf.iloc[i].sort_values(ascending=False)[0:3])
#     gavno.append( 'KWP' + str(i) + ': ' + ', '.join(lssst[i].index))
# gavno = ' '.join(gavno)

# dff = pd.DataFrame()
# dff['Res'] = gavno
# dff
# df = pd.DataFrame([[1,'Bob', 'Builder'],
#                   [2,'Sally', 'Baker'],
#                   [3,'Scott', 'Candle Stick Maker']],
# columns=['id','name', 'occupation'])
# df
# shluha.append(shluha_2)
# sf = pd.DataFrame()
# sf['Trah'] = ()
# sf['Trah'].append(gavno)
# gavno = '. '.join(gavno)
# # # gavno
# Dagg = pd.DataFrame()
# Dagg['Trah'] = Dagg.append(gavno)
# # Dagg['Trah'] = gavno
# print((Dagg['Trah']))
# Dagg['Trah'].append(gavno_2)
# Dagg['Trah'] = gavno
# Dagg


# df_ga['HUI'] = lssst
# df_ga
# lssst[0].values
# lssst[0].index
# print(type(lssst[0]))
    # print(df_tfidf.iloc[i].sort_values(ascending=False)[0:5])
#
# vectorizer_2 = TfidfVectorizer(analyzer='word', ngram_range=(2,2), stop_words = stop_words)
# vectorizer_2.fit(bebra)
# tfidf_2 = vectorizer_2.transform(bebra)
# vectors_2 = vectorizer_2.get_feature_names()
# smatrix_2 = vectorizer_2.transform(bebra)
# dense_2 = smatrix_2.todense()
# dense_list_2 = dense_2.tolist()
# df_tfidf_2 = pd.DataFrame(dense_list_2,columns=vectors_2)
# print(df_tfidf_2);
# s = df_tfidf.iloc[0]
# s.sort_values(ascending=False)